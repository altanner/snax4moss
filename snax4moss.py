
#~ Standard library imports
import sys
import time
import datetime
import re
from random import randint

#~ 3rd party imports
import requests
from pathlib import Path
from retry import retry
import pandas as pd
from bs4 import BeautifulSoup
from alive_progress import alive_bar
import lxml      # alt to html.parser, with cchardet >> speed up
import cchardet  # character recognition

#~ Local imports
import targets       # this will be the template in published version
import scraper_meta  # things like user agents, in case we need to rotate


def get_category_links(category, baseurl) -> pd.Series:
    """Get the full URL for each sample in a category

    Args: a category and baseURL from targets.py
    Rets: a pandas series of the product URLs"""

    links = []
    #~ the final section of the category URL
    category_name = category.split("/")[-1]

    #~ pull down the category page
    category_page = baseurl + category
    try:
        target = requests.get(
            category_page, headers=scraper_meta.user_agent).text
    except Exception as e:
        print(f"This request threw an error: {e}")
    #~ init BS object
    soup = BeautifulSoup(target, "lxml")

    #~ retrieve the link text element for all samples in this category
    #! find proper div
    sample_list = soup.find_all(
        "a", {"class": "product_name_link product_view_gtm"})

    print(f"{len(links)} links retrieved from category {category_name}")

    #~ add to a list of the href URLs
    with alive_bar() as bar:
        for sample in sample_list:
            link = sample.get("href")
            links.append(link)
            bar()  #~ increment progress bar

    #~ turn the list into a series and return
    linx = pd.Series(links, dtype=object)
    return linx


def make_dataframe_of_links_from_all_categories(start_time,
                                                categories) -> pd.DataFrame:
    """
    Rets: Dataframe with first column as sample URLs
    """

    all_links = pd.Series(dtype=str)
    print("\n" + f".oO Finding links for {len(categories)} product categories")

    for category in categories:
        product_links = get_links_from_one_category(category, targets.baseurl)
        all_links = all_links.append(product_links, ignore_index=True)

    #~ clean dups and re-index
    all_links = all_links.drop_duplicates().reset_index(drop=True)
    #~ send series to DF
    all_links = all_links.to_frame()
    #~ label column one
    all_links.columns = ["product_link"]

    return all_links


def populate_links_df_with_extracted_fields(dataframe,
                                            fields_to_extract,
                                            start_time) -> pd.DataFrame:
    """
    Takes a dataframe where column 0 = URLs, generated by
    the get links function. Puts the contents of each
    field of interest into a dataframe.

    Args: dataframe column 0 = a URL,
          list of fields (a list of lists)
    Rets: populated dataframe
    """

    total_snax = len(fields_to_extract) * dataframe.shape[0]
    regex = re.compile(r"[\n\r\t]+")  #~ whitespace cleaner
    print("\n" + f".oO Retreiving details for {dataframe.shape[0]} products")

    with alive_bar(dataframe.shape[0]) as bar:

        for index in range(dataframe.shape[0]):

            @retry(ConnectionResetError, tries=3, delay=10, backoff=10)
            def get_target_page(index):
                #~ pull down the full product page
                return requests.get(dataframe.at[index, "product_link"], headers=scraper_meta.user_agent).text

            try:
                target = get_target_page(index)
            except Exception as e:  #~ try giving the server a break
                print(
                    "\n" + f".oO Issue getting page: {e}, might be server-side, sleeping for 15 minutes.")
                time.sleep(900)
                continue

            #~ init BSoup object
            soup = BeautifulSoup(target, "lxml")

            for field in fields_to_extract:

                field_value = ""
                try:
                    if field[0] == "multi":  #~ nested aquire from "Product details" div
                        try:
                            for element in soup.find_all(
                                    field[1], attrs={field[2]: field[3]}):
                                field_value += str([thing for thing in element.get_text(
                                    separator=" ").splitlines() if thing])
                        except Exception as e:
                            print(f"Field \"{field[3]}\" not found", e)
                            continue
                    else:  #~ just get the target field
                        field_value = soup.find(
                            field[1], attrs={field[2]: field[3]}).get_text(strip=True)
                except AttributeError:
                    print(f"Field \"{field[3]}\" not found")
                    continue

                #~ clean up space, lines, wordsstickingtogether, odd chars.
                clean_field = re.sub(
                    "\[|\]|\"|'|,|...read more|...read less", "", field_value)
                clean_field = re.sub(r"\\t", "", clean_field)
                clean_field = re.sub(r"\\xa0", "", clean_field)
                clean_field = re.sub(r"\s+", " ", clean_field)
                dataframe.loc[index, field[3]] = clean_field

                # time.sleep(randint(1, 3))  #~ relax a little

            bar()

    return dataframe


def select_long_description_field(dataframe) -> pd.DataFrame:
    """Columns named 13 and 14 and called that because
    those are the nested div names on the boots website;
    it is unclear which will be the true field, so both are acquired.
    We need to take the longer field, the shorter always being PDF or
    ordering details, or other crap that we don't want."""

    print(
        f".oO IDing long_description field for {dataframe.shape[0]} products")

    with alive_bar() as bar:
        for index in range(dataframe.shape[0]):

            #~ compare fields
            longer_field = max(
                [dataframe.iloc[index]["13"]],
                [dataframe.iloc[index]["14"]])
            dataframe.loc[index, "long_description"] = longer_field
            bar()

    #~ remove candidate fields
    dataframe = dataframe.drop(["13", "14"], axis=1)

    return dataframe


def main():

    try:
        start_time = datetime.datetime.now().replace(microsecond=0).isoformat()
        start_counter = time.perf_counter()

        print(
            f"\n.oO Starting snax2 @ {start_time} - target base URL is {targets.baseurl}")

        snax = make_dataframe_of_links_from_all_categories(start_time,
                                                           targets.categories)
        if snax.empty:
            print(f"\n.oO No links retrieved... Stopping.")
            sys.exit(1)

        #~ make output folder, if not there
        Path("./output").mkdir(parents=False, exist_ok=True)
        snax.to_csv("output/linx_" + start_time + ".csv")  #~ save links

        snax = populate_links_df_with_extracted_fields(
            snax,
            targets.fields_to_extract,
            start_time)
        snax = select_long_description_field(snax)
        snax.to_csv("output/snax_" + start_time + ".csv")  #~ save full output

        end_time = datetime.datetime.now().replace(microsecond=0).isoformat()
        end_counter = time.perf_counter()
        elapsed = datetime.timedelta(seconds=(end_counter - start_counter))

        print(f".oO OK, finished scrape @ {end_time}, taking {elapsed}")

    except KeyboardInterrupt:
        print("\n.oO OK, dropping. That run was not saved.")
        sys.exit(0)


if __name__ == "__main__":

    main()
